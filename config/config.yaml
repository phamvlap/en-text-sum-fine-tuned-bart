# General configs
seed: 42

# Model configs
model_name_or_path: "facebook/bart-base"

# Preprocess dataset configs
dataset_dir: dataset
data_files_path:
  raw: dataset/raw.csv
  train: dataset/train.csv
  val: dataset/val.csv
  test: dataset/test.csv
datasource_path: path/to/datasource
original_text_src: text
original_text_tgt: headline
text_src: article
text_tgt: abstract

# Split dataset configs
train_size: 0.75
val_size: 0.1
test_size: 0.15
is_sampling: False
num_samples: 1000
lowercase: True
contractions: True
remove_invalid_length: True
remove_invalid_text: True
shuffle: True

# Tokenizer configs
tokenizer_train_ds_path: dataset/raw.csv
tokenizer_bart_dir: tokenizer-bart
is_train_tokenizer: True
special_tokens:
  - <s>
  - </s>
  - <pad>
  - <unk>
  - <mask>
shared_vocab: True
vocab_size: 50265
min_freq: 2
model_type: byte_level_bpe
show_progress: True

# Dataloader configs
batch_size_train: 32
batch_size_val: 16
batch_size_test: 4
shuffle_dataloader: True
num_workers: 2
attach_text: False

# Adam and AdamW optimizer configs
optimizer: adamw
lr: 0.3
betas:
  - 0.9
  - 0.98
eps: 1e-9 # = 10 ** -9
weight_decay: 0.0

# Learning rate scheduler configs
lr_scheduler: noam # noam | cosine

# NoamLR scheduler configs
warmup_steps: 4000

# CosineAnnealing with warm restarts scheduler configs
T_0: 10
T_mult: 2
eta_min: 1e-5

# Loss function configs
label_smoothing: 0.1

# Training configs
checkpoint_dir: checkpoints
model_basename: bart_model
epochs: 10
eval_every_n_steps: 1000
save_every_n_steps: 5000
max_grad_norm: 1.0
f16_precision: True
max_eval_steps: 100
max_train_steps: -1
max_saved_checkpoints: 3
show_eval_progress: False
resume_from_checkpoint: True

# BART configs
seq_length: 512
src_seq_length: 768 # max length of input sequence
tgt_seq_length: 256 # max length of output sequence
d_model: 768 # dimension of hidden layers

# Rouge score configs
rouge_keys:
  - rouge1
  - rouge2
  - rougeL
use_stemmer: True
accumulate: avg # 'best' | 'avg'
log_examples: True
logging_steps: 20

# Beam search configs
beam_size: 3
topk: 2

# Statistics result configs
statistic_dir: statistics

# Wandb writer configs
is_logging_wandb: True
wandb_project_name: en-text-sum-fine-tuned-bart
wandb_log_dir: wandb-logs
wandb_key: wandb-key

# Push to hub
push_to_hub: True
hub_repo_name: text-summarization-finetuned-bart

# Distributed Data Parallel training configs
use_ddp: False
rank: -1
local_rank: -1
world_size: 0
